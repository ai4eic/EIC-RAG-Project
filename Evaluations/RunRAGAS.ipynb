{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359635bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import toml, os, sys\n",
    "\n",
    "sys.path.append(os.path.realpath(\"../\"))\n",
    "\n",
    "\n",
    "# set the API key as environment variable\n",
    "langchain_api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "print(langchain_api_key)\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "# os.environ[\"LANGCHAIN_ENDPOINT\"] = secrets[\"LANGCHAIN_ENDPOINT\"]\n",
    "print(f\"langchain_api_key :\" , {langchain_api_key})\n",
    "\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINECONE_API_KEY\") #secrets[\"PINECONE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1c1475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get the dataset\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0c13672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"AI4EIC_sample_dataset.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_chroma import Chroma\n",
    "from streamlit_app.app_utilities import *\n",
    "from streamlit_app.LangChainUtils.LLMChains import *\n",
    "from langchain import callbacks\n",
    "from langsmith import Client\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from ragas.run_config import RunConfig\n",
    "\n",
    "\n",
    "# def RunQuery(input_question, max_k, sim_score):\n",
    "def RunQuery(input_question, max_k, sim_score,\n",
    "             collection_name=None, db_name=None, table_name=None):\n",
    "    \n",
    "    #for generating output form prompt\n",
    "    llm = ChatOllama(model=\"llama3.2:latest\", temperature=0, num_predict=4096)\n",
    "\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "\n",
    "    # Defining some props of DB\n",
    "    SimilarityDict = {\"Cosine similarity\" : \"similarity\", \"MMR\" : \"mmr\"}\n",
    "\n",
    "    # DBProp = {\"PINECONE\" : {\"vector_config\" : {\"db_api_key\" : secrets[\"PINECONE_API_KEY\"], \n",
    "    DBProp = {\"PINECONE\" : {\"vector_config\" : {\"db_api_key\" :pinecone_api_key, \n",
    "                                                \"index_name\" : \"llm-project\", \n",
    "                                                \"embedding_function\" : embeddings\n",
    "                                                },\n",
    "                            \"search_config\" : {\"metric\" : sim_score, \n",
    "                                            \"search_kwargs\" : {\"k\" : max_k}\n",
    "                                            },\n",
    "                            \"available_metrics\" : [\"Cosine similarity\", \"MMR\"]\n",
    "                            },\n",
    "                \"CHROMA\" : {\"vector_config\" : {\"db_name\" : db_name, \n",
    "                                                \"embedding_function\" : embeddings, \n",
    "                                                \"collection_name\": collection_name\n",
    "                                                },\n",
    "                            \"search_config\" : {\"metric\" : sim_score, \n",
    "                                            \"search_kwargs\" : {\"k\" : max_k}\n",
    "                                            },\n",
    "                            \"available_metrics\" : [\"Cosine similarity\", \"MMR\"]\n",
    "                            },\n",
    "                \"LANCE\" : {\"vector_config\" : {\"db_name\" : db_name, \n",
    "                                                \"table_name\": table_name\n",
    "                                                },\n",
    "                            \"search_config\" : {\"metric\" : sim_score, \n",
    "                                            \"search_kwargs\" : {\"k\" : max_k}\n",
    "                                            },\n",
    "                            \"available_metrics\" : [\"Cosine similarity\", \"MMR\"]\n",
    "                            },\n",
    "            }\n",
    "    # retriever = GetRetriever(\"PINECONE\", DBProp[\"PINECONE\"][\"vector_config\"], DBProp[\"PINECONE\"][\"search_config\"])\n",
    "    retriever = GetRetriever(\"CHROMA\", DBProp[\"CHROMA\"][\"vector_config\"], DBProp[\"CHROMA\"][\"search_config\"])\n",
    "    print(\"output of Getretriever\")\n",
    "    # project name for tracing in Langsmith\n",
    "    project_name = f\"RAG-CHAT-tapasi\"\n",
    "\n",
    "    tracer = LangChainTracer(project_name = project_name)\n",
    "    print(\"out of LangChainTracer\")\n",
    "    run_name = \"Evaluation-testings\"\n",
    "    trace_metadata = {\"DBType\": \"CHROMA\", \n",
    "                    \"similarity_score\": sim_score, \n",
    "                    \"max_k\": max_k\n",
    "                    }\n",
    "    RUNCHAIN = RunChatBot(llm, retriever, \"../Templates\"\n",
    "                        ).with_config({\"callbacks\": [tracer], \n",
    "                                        \"run_name\": run_name,\n",
    "                                        \"metadata\": trace_metadata}\n",
    "                                        )\n",
    "    print(\"out of RunCHatBot\")\n",
    "    trace_id = \"\"\n",
    "    response = \"\"\n",
    "    runid = \"\"\n",
    "    # run the chain with tracing enabled when os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    with tracing_v2_enabled(project_name) as cb:\n",
    "        with callbacks.collect_runs() as ccb:\n",
    "            output = RUNCHAIN.invoke(input_question)\n",
    "            response = output[\"answer\"]\n",
    "            print (output)\n",
    "            print (len(ccb.traced_runs))\n",
    "            for run in ccb.traced_runs:\n",
    "                runid = run.id\n",
    "                print (run.name)\n",
    "                print (run.id)\n",
    "                print (run.inputs)\n",
    "                print (run.outputs)\n",
    "                print (run.trace_id)\n",
    "                trace_id = run.trace_id\n",
    "    return response, trace_id, client.share_run(runid)\n",
    "\n",
    "# use this function to generate answers when llm is not used through Chain \n",
    "def RunLLM(input_question, MODEL = \"llama3.2:latest\"):\n",
    "    # model_name = f\"gpt-3.5-turbo-1106\" if GPTMODEL == 3 else \"gpt-4-0125-preview\"\n",
    "    print (f\"input_question, {input_question}\")\n",
    "    # llm = ChatOpenAI(model_name=model_name, temperature=0,\n",
    "    #                 max_tokens = 4096\n",
    "    #                 )\n",
    "    llm = ChatOllama(model_name=MODEL, temperature=0, num_predict=4096)\n",
    "    output = llm.invoke(input_question).content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e414e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datasets import Dataset\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "# from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "\n",
    "import ragas\n",
    "evaluator_embeddings = OpenAIEmbeddings()\n",
    "# wrapper is used due to a different name of the model instead of the original name \"gpt-4o-mini\"\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini-2024-07-18\"))\n",
    "# embeddings = OllamaEmbeddings(model = \"mxbai-embed-large:latest\")\n",
    "\n",
    "'''RAGAS metrics uses openai models by default. So we explicitly define llm and embedding model of ollama and use to\n",
    "compute evaluation metrics'''\n",
    "\n",
    "# ollama_embeddings = OllamaEmbeddings(model=\"mxbai-embed-large:latest\")\n",
    "# ollama_llm = ChatOllama(model=\"llama3.2:latest\", temperature=0)\n",
    "\n",
    "# # wrap the above defined llm and embedding model\n",
    "# wrapped_llm = LangchainLLMWrapper(ollama_llm)\n",
    "# wrapped_embeddings = LangchainEmbeddingsWrapper(ollama_embeddings)\n",
    "\n",
    "ANSWER_CORRECTNESS = ragas.metrics.AnswerCorrectness(name = \"ANSWER_CORRECTNESS\",\n",
    "                                                     weights = [0.90, 0.10],\n",
    "                                                    #  llm = wrapped_llm,\n",
    "                                                    #  embeddings = wrapped_embeddings\n",
    "                                                     )\n",
    "ANSWER_RELEVANCY = ragas.metrics.AnswerRelevancy(name = \"ANSWER_RELEVANCY\",\n",
    "                                                strictness = 5,\n",
    "                                                # llm = wrapped_llm,\n",
    "                                                # embeddings = wrapped_embeddings\n",
    "                                                )\n",
    "CONTEXT_ENTITY_RECALL = ragas.metrics.ContextEntityRecall(name = \"CONTEXT_ENTITY_RECALL\",\n",
    "                                                        #   llm = wrapped_llm\n",
    "                                                         )\n",
    "CONTEXT_PRECISION = ragas.metrics.ContextPrecision(name = \"CONTEXT_PRECISION\",\n",
    "                                                #    llm = wrapped_llm\n",
    "                                                    )\n",
    "CONTEXT_RECALL = ragas.metrics.ContextRecall(name = \"CONTEXT_RECALL\",\n",
    "                                            #  llm = wrapped_llm\n",
    "                                            )\n",
    "## new RAGAS doesn't have this evaluation metric                                             \n",
    "# CONTEXT_RELEVANCY = ragas.metrics.ContextRelevancy(name = \"CONTEXT_RELEVANCY\")\n",
    "                                                   \n",
    "FAITHFULNESS = ragas.metrics.Faithfulness(name = \"FAITHFULNESS\",\n",
    "                                            )\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the benchmark Q&A dataframe\n",
    "df = pd.read_csv(\"AI4EIC_sample_dataset.csv\", sep = \",\")\n",
    "\n",
    "\n",
    "from ragas import evaluate\n",
    "#output file to store answers from RAG system and will be used for RAGAS evaluation\n",
    "dataset = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": [], \"arxiv_id\": [], \"input_arxiv_id\": [], \"trace_links\": []}\n",
    "max_k = 3\n",
    "sim_score = \"mmr\"\n",
    "db_name=\"../ingestion/myChromaDB\"\n",
    "collection_name = \"EIC_archive\"\n",
    "table_name = \"arxiv_table\"\n",
    "if (os.path.exists(f\"results_k_{max_k}_sim_{sim_score}.csv\")):\n",
    "    os.system(f\"rm -f results_k_{max_k}_sim_{sim_score}.csv\")\n",
    "\n",
    "# Iterate through the benchmark Q&A dataframe and run the query for each question\n",
    "for index, row in df.iterrows():\n",
    "    question = row[\"input_question\"]\n",
    "\n",
    "    # Run the query, generate answer through llm\n",
    "    answer, trace_id, trace_link = RunQuery(question, max_k, sim_score, db_name=db_name, collection_name=collection_name)\n",
    "    print(f\" RunQuery : answer : {answer}, trace_id : {trace_id}, trace_link : {trace_link}\")\n",
    "\n",
    "    project_name = f\"RAG-CHAT-tapasi\"\n",
    "    run_name = \"Evaluation-testings\"\n",
    "    # print(f\"before langsmith is called\")\n",
    "    runs = client.list_runs(project_name = project_name, trace_id = trace_id)\n",
    "    # print(f\"after langsmith client is called : , {runs}\")\n",
    "    contexts = []\n",
    "    cite_arxiv_ids = []\n",
    "    for run in runs:\n",
    "        if (run.run_type.lower() == \"retriever\"):\n",
    "            print (run.name)\n",
    "            print (run.id)\n",
    "            print (run.inputs)\n",
    "            print (run.outputs)\n",
    "            for i in run.outputs['documents']:\n",
    "                contexts.append(i[\"page_content\"])\n",
    "                cite_arxiv_ids.append(i[\"metadata\"][\"arxiv_id\"].split(\"/\")[-1].strip())\n",
    "            print (run.trace_id)\n",
    "            print (\"-----\")\n",
    "    dataset[\"question\"].append(question)\n",
    "    print (answer.split(\"http://\")[0].strip(\"\\n\"))\n",
    "    dataset[\"answer\"].append(answer.split(\"http://\")[0].strip(\"\\n\"))\n",
    "    dataset[\"contexts\"].append(contexts)\n",
    "    dataset[\"ground_truth\"].append(row[\"output_complete_response\"])\n",
    "    dataset[\"input_arxiv_id\"].append(row[\"input_arxiv_id\"])\n",
    "    dataset[\"arxiv_id\"].append(cite_arxiv_ids)\n",
    "    dataset[\"trace_links\"].append(trace_link)\n",
    "    \n",
    "    with open(f\"dataset_k_{max_k}_sim_{sim_score}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    \n",
    "    tmpdataset = {}\n",
    "    for key, value in dataset.items():\n",
    "        tmpdataset[key] = [value[-1]]\n",
    "    DATASET = Dataset.from_dict(tmpdataset)\n",
    "    \n",
    "\n",
    "    result = evaluate(DATASET,\n",
    "                  metrics = [\n",
    "                      FAITHFULNESS,\n",
    "                    #   CONTEXT_RELEVANCY,\n",
    "                      CONTEXT_ENTITY_RECALL,\n",
    "                      CONTEXT_PRECISION,\n",
    "                      CONTEXT_RECALL,\n",
    "                      ANSWER_RELEVANCY,\n",
    "                      ANSWER_CORRECTNESS\n",
    "                  ],\n",
    "                llm = evaluator_llm,\n",
    "                embeddings = evaluator_embeddings,\n",
    "                  )\n",
    "    result_df = result.to_pandas()\n",
    "    if (os.path.exists(f\"results_k_{max_k}_sim_{sim_score}.csv\")):\n",
    "        df = pd.read_csv(f\"results_k_{max_k}_sim_{sim_score}.csv\", sep = \",\")\n",
    "        result_df = pd.concat([df, result_df])\n",
    "    result_df.to_csv(f\"results_k_{max_k}_sim_{sim_score}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa339e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_RAG-EIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
