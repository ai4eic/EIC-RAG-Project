{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating RAGAS for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml, os, sys\n",
    "\n",
    "sys.path.append(os.path.realpath(\"../\"))\n",
    "\n",
    "with open(\"../.streamlit/secrets.toml\") as f:\n",
    "    secrets = toml.load(f)\n",
    "    \n",
    "os.environ[\"OPENAI_API_KEY\"] = secrets[\"OPENAI_API_KEY\"]\n",
    "if secrets.get(\"LANGCHAIN_API_KEY\"):\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = secrets[\"LANGCHAIN_API_KEY\"]\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = secrets[\"LANGCHAIN_ENDPOINT\"]\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = secrets[\"PINECONE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach to the existing event loop when using jupyter notebooks\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get the dataset\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithError\n",
    "\n",
    "client = Client()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"AI4EIC2023_DATASETS.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from streamlit_app.app_utilities import *\n",
    "from streamlit_app.LangChainUtils.LLMChains import *\n",
    "from langchain import callbacks\n",
    "from langsmith import Client\n",
    "from langchain_core.tracers.context import tracing_v2_enabled\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "\n",
    "def RunQuery(input_question, max_k, sim_score):\n",
    "    \n",
    "    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-1106\", temperature=0,\n",
    "                    max_tokens = 4096\n",
    "                    )\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # Defining some props of DB\n",
    "    SimilarityDict = {\"Cosine similarity\" : \"similarity\", \"MMR\" : \"mmr\"}\n",
    "\n",
    "    DBProp = {\"PINECONE\" : {\"vector_config\" : {\"db_api_key\" : secrets[\"PINECONE_API_KEY\"], \n",
    "                                                \"index_name\" : \"llm-project\", \n",
    "                                                \"embedding_function\" : embeddings\n",
    "                                                },\n",
    "                            \"search_config\" : {\"metric\" : sim_score, \n",
    "                                            \"search_kwargs\" : {\"k\" : max_k}\n",
    "                                            },\n",
    "                            \"available_metrics\" : [\"Cosine similarity\", \"MMR\"]\n",
    "                            },\n",
    "            }\n",
    "    retriever = GetRetriever(\"PINECONE\", DBProp[\"PINECONE\"][\"vector_config\"], DBProp[\"PINECONE\"][\"search_config\"])\n",
    "    project_name = f\"RAG-CHAT-ksuresh\"\n",
    "    tracer = LangChainTracer(project_name = project_name)\n",
    "    run_name = \"Evaluation-testings\"\n",
    "    trace_metadata = {\"DBType\": \"PINECONE\", \n",
    "                    \"similarity_score\": sim_score, \n",
    "                    \"max_k\": max_k\n",
    "                    }\n",
    "    RUNCHAIN = RunChatBot(llm, retriever, \"/mnt/d/LLM-Project/EIC-RAG-Project/Templates\"\n",
    "                        ).with_config({\"callbacks\": [tracer], \n",
    "                                        \"run_name\": run_name,\n",
    "                                        \"metadata\": trace_metadata}\n",
    "                                        )\n",
    "    trace_id = \"\"\n",
    "    response = \"\"\n",
    "    runid = \"\"\n",
    "    with tracing_v2_enabled(project_name) as cb:\n",
    "        with callbacks.collect_runs() as ccb:\n",
    "            output = RUNCHAIN.invoke(input_question)\n",
    "            response = output[\"answer\"]\n",
    "            print (output)\n",
    "            print (len(ccb.traced_runs))\n",
    "            for run in ccb.traced_runs:\n",
    "                runid = run.id\n",
    "                print (run.name)\n",
    "                print (run.id)\n",
    "                print (run.inputs)\n",
    "                print (run.outputs)\n",
    "                print (run.trace_id)\n",
    "                trace_id = run.trace_id\n",
    "    return response, trace_id, client.share_run(runid)\n",
    "\n",
    "def RunLLM(input_question, GPTMODEL = 3):\n",
    "    model_name = f\"gpt-3.5-turbo-1106\" if GPTMODEL == 3 else \"gpt-4-0125-preview\"\n",
    "    print (input_question)\n",
    "    llm = ChatOpenAI(model_name=model_name, temperature=0,\n",
    "                    max_tokens = 4096\n",
    "                    )\n",
    "    output = llm.invoke(input_question).content\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datasets import Dataset\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "import ragas\n",
    "embeddings = OpenAIEmbeddings()\n",
    "ANSWER_CORRECTNESS = ragas.metrics.AnswerCorrectness(name = \"ANSWER_CORRECTNESS\",\n",
    "                                                     weights = [0.90, 0.10]\n",
    "                                                     )\n",
    "ANSWER_RELEVANCY = ragas.metrics.AnswerRelevancy(name = \"ANSWER_RELEVANCY\",\n",
    "                                                strictness = 5\n",
    "                                                )\n",
    "CONTEXT_ENTITY_RECALL = ragas.metrics.ContextEntityRecall(name = \"CONTEXT_ENTITY_RECALL\"\n",
    "                                                         )\n",
    "CONTEXT_PRECISION = ragas.metrics.ContextPrecision(name = \"CONTEXT_PRECISION\"\n",
    "                                                    )\n",
    "CONTEXT_RECALL = ragas.metrics.ContextRecall(name = \"CONTEXT_RECALL\"\n",
    "                                             )\n",
    "CONTEXT_RELEVANCY = ragas.metrics.ContextRelevancy(name = \"CONTEXT_RELEVANCY\"\n",
    "                                                   )\n",
    "FAITHFULNESS = ragas.metrics.Faithfulness(name = \"FAITHFULNESS\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"AI4EIC2023_DATASETS.csv\", sep = \",\")\n",
    "from ragas import evaluate\n",
    "dataset = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": [], \"arxiv_id\": [], \"input_arxiv_id\": [], \"trace_links\": []}\n",
    "max_k = 3\n",
    "sim_score = \"mmr\"\n",
    "if (os.path.exists(f\"results_k_{max_k}_sim_{sim_score}.csv\")):\n",
    "    os.system(f\"rm -f results_k_{max_k}_sim_{sim_score}.csv\")\n",
    "for index, row in df.iterrows():\n",
    "    question = row[\"input_question\"]\n",
    "    answer, trace_id, trace_link = RunQuery(question, max_k, sim_score)\n",
    "\n",
    "    project_name = f\"RAG-CHAT-ksuresh\"\n",
    "    run_name = \"Evaluation-testings\"\n",
    "    runs = client.list_runs(project_name = project_name, trace_id = trace_id)\n",
    "    contexts = []\n",
    "    cite_arxiv_ids = []\n",
    "    for run in runs:\n",
    "        if (run.run_type.lower() == \"retriever\"):\n",
    "            print (run.name)\n",
    "            print (run.id)\n",
    "            print (run.inputs)\n",
    "            print (run.outputs)\n",
    "            for i in run.outputs['documents']:\n",
    "                contexts.append(i[\"page_content\"])\n",
    "                cite_arxiv_ids.append(i[\"metadata\"][\"arxiv_id\"].split(\"/\")[-1].strip())\n",
    "            print (run.trace_id)\n",
    "            print (\"-----\")\n",
    "    dataset[\"question\"].append(question)\n",
    "    print (answer.split(\"http://\")[0].strip(\"\\n\"))\n",
    "    dataset[\"answer\"].append(answer.split(\"http://\")[0].strip(\"\\n\"))\n",
    "    dataset[\"contexts\"].append(contexts)\n",
    "    dataset[\"ground_truth\"].append(row[\"output_complete_response\"])\n",
    "    dataset[\"input_arxiv_id\"].append(row[\"input_arxiv_id\"])\n",
    "    dataset[\"arxiv_id\"].append(cite_arxiv_ids)\n",
    "    dataset[\"trace_links\"].append(trace_link)\n",
    "    \n",
    "    with open(f\"dataset_k_{max_k}_sim_{sim_score}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(dataset, f)\n",
    "    \n",
    "    tmpdataset = {}\n",
    "    for key, value in dataset.items():\n",
    "        tmpdataset[key] = [value[-1]]\n",
    "    DATASET = Dataset.from_dict(tmpdataset)\n",
    "    \n",
    "    result = evaluate(DATASET,\n",
    "                  metrics = [\n",
    "                      FAITHFULNESS,\n",
    "                      CONTEXT_RELEVANCY,\n",
    "                      CONTEXT_ENTITY_RECALL,\n",
    "                      CONTEXT_PRECISION,\n",
    "                      CONTEXT_RECALL,\n",
    "                      ANSWER_RELEVANCY,\n",
    "                      ANSWER_CORRECTNESS\n",
    "                  ]\n",
    "                  )\n",
    "    result_df = result.to_pandas()\n",
    "    if (os.path.exists(f\"results_k_{max_k}_sim_{sim_score}.csv\")):\n",
    "        df = pd.read_csv(f\"results_k_{max_k}_sim_{sim_score}.csv\", sep = \",\")\n",
    "        result_df = pd.concat([df, result_df])\n",
    "    result_df.to_csv(f\"results_k_{max_k}_sim_{sim_score}.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from datasets import Dataset\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "\n",
    "from ragas import evaluate\n",
    "dataset3 = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": [], \"arxiv_id\": [], \"input_arxiv_id\": [], \"trace_links\": []}\n",
    "dataset4 = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": [], \"arxiv_id\": [], \"input_arxiv_id\": [], \"trace_links\": []}\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"AI4EIC2023_DATASETS.csv\", sep = \",\")\n",
    "\n",
    "if (os.path.exists(f\"results_gpt3.csv\")):\n",
    "    os.system(f\"rm -f results_gpt3.csv\")\n",
    "if (os.path.exists(f\"results_gpt4.csv\")):\n",
    "    os.system(f\"rm -f results_gpt4.csv\")\n",
    "for index, row in df.iterrows():\n",
    "    question = row[\"input_question\"]\n",
    "    output3 = RunLLM(question, GPTMODEL = 3)\n",
    "    output4 = RunLLM(question, GPTMODEL = 4)\n",
    "    dataset3[\"question\"].append(question)\n",
    "    dataset3[\"answer\"].append(output3)\n",
    "    dataset3[\"contexts\"].append([row[\"output_complete_response\"]])\n",
    "    dataset3[\"ground_truth\"].append(row[\"output_complete_response\"])\n",
    "    dataset3[\"input_arxiv_id\"].append(row[\"input_arxiv_id\"])\n",
    "    dataset3[\"arxiv_id\"].append([\"\"])\n",
    "    dataset3[\"trace_links\"].append(\"\")\n",
    "    \n",
    "    dataset4[\"question\"].append(question)\n",
    "    dataset4[\"answer\"].append(output4)\n",
    "    dataset4[\"contexts\"].append([row[\"output_complete_response\"]])\n",
    "    dataset4[\"ground_truth\"].append(row[\"output_complete_response\"])\n",
    "    dataset4[\"input_arxiv_id\"].append(row[\"input_arxiv_id\"])\n",
    "    dataset4[\"arxiv_id\"].append([\"\"])\n",
    "    dataset4[\"trace_links\"].append(\"\")\n",
    "    \n",
    "    tmpdataset3 = {}\n",
    "    for key, value in dataset3.items():\n",
    "        tmpdataset3[key] = [value[-1]]\n",
    "    DATASET3 = Dataset.from_dict(tmpdataset3)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    result3 = evaluate(DATASET3,\n",
    "                    metrics = [\n",
    "                        FAITHFULNESS,\n",
    "                        CONTEXT_RELEVANCY,\n",
    "                        CONTEXT_ENTITY_RECALL,\n",
    "                        CONTEXT_PRECISION,\n",
    "                        CONTEXT_RECALL,\n",
    "                        ANSWER_RELEVANCY,\n",
    "                        ANSWER_CORRECTNESS\n",
    "                    ],\n",
    "                    embeddings = embeddings\n",
    "                    )\n",
    "    result_df3 = result3.to_pandas()\n",
    "    if (os.path.exists(f\"results_gpt3.csv\")):\n",
    "        df = pd.read_csv(f\"results_gpt3.csv\", sep = \",\")\n",
    "        result_df3 = pd.concat([df, result_df3])\n",
    "    result_df3.to_csv(f\"results_gpt3.csv\", index = False)\n",
    "    \n",
    "    tmpdataset4 = {}\n",
    "    for key, value in dataset4.items():\n",
    "        tmpdataset4[key] = [value[-1]]\n",
    "    DATASET4 = Dataset.from_dict(tmpdataset4)\n",
    "    \n",
    "    result4 = evaluate(DATASET4,\n",
    "                    metrics = [\n",
    "                        FAITHFULNESS,\n",
    "                        CONTEXT_RELEVANCY,\n",
    "                        CONTEXT_ENTITY_RECALL,\n",
    "                        CONTEXT_PRECISION,\n",
    "                        CONTEXT_RECALL,\n",
    "                        ANSWER_RELEVANCY,\n",
    "                        ANSWER_CORRECTNESS\n",
    "                    ],\n",
    "                    embeddings = embeddings\n",
    "                    )\n",
    "    result_df4 = result4.to_pandas()\n",
    "    if (os.path.exists(f\"results_gpt4.csv\")):\n",
    "        df = pd.read_csv(f\"results_gpt4.csv\", sep = \",\")\n",
    "        result_df4 = pd.concat([df, result_df4])\n",
    "    result_df4.to_csv(f\"results_gpt4.csv\", index = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "max_k = 3\n",
    "sim_score = \"mmr\"\n",
    "%matplotlib inline\n",
    "def analysis(df_list, labels = [\"RAG4EIC\"]):\n",
    "  sns.set_style(\"whitegrid\")\n",
    "  df = df_list[0]\n",
    "  fig, axs = plt.subplots(df.columns.size, 1, figsize=(5, 5*df.columns.size))\n",
    "  colors = [\"orange\", \"blue\", \"green\"]\n",
    "  for i,col in enumerate(df.columns):\n",
    "    #sns.kdeplot(data=[df[col].values],legend=False,ax=axs[i],fill=True)\n",
    "    sns.histplot(data=[df[col].values for df in df_list], kde=True, ax=axs[i], color = colors, alpha = 0.5)\n",
    "    axs[i].set_title(f'{col} scores distribution')\n",
    "    axs[i].legend(labels=labels)\n",
    "    axs[i].set_xlabel(\"Score\")\n",
    "    axs[i].set_xlim(0., 1.1)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "#columns = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', 'answer_correctness']\n",
    "#FAITHFULNESS,CONTEXT_RELEVANCY,CONTEXT_ENTITY_RECALL,CONTEXT_PRECISION,CONTEXT_RECALL,ANSWER_RELEVANCY,ANSWER_CORRECTNESS\n",
    "columns = [\"FAITHFULNESS\", \"CONTEXT_RELEVANCY\", \"CONTEXT_ENTITY_RECALL\", \"CONTEXT_PRECISION\", \"CONTEXT_RECALL\", \"ANSWER_RELEVANCY\", \"ANSWER_CORRECTNESS\"]\n",
    "result_df = pd.read_csv(f\"results_k_{max_k}_sim_{sim_score}.csv\", sep = \",\")[columns]\n",
    "result_df.fillna(0, inplace = True)\n",
    "gpt3_df = pd.read_csv(f\"results_gpt3.csv\", sep = \",\")[columns]\n",
    "gpt3_df.fillna(0, inplace = True)\n",
    "#gpt4_df = pd.read_csv(f\"results_gpt3.csv\", sep = \",\")[columns]\n",
    "analysis(\n",
    "    [result_df, gpt3_df],\n",
    "    [\"RAG4EIC\", \"GPT3\"]\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.plot(kind = \"box\", figsize = (20, 5), title = \"RAG4EIC\")\n",
    "gpt3_df.plot(kind = \"box\", figsize = (20, 5), title = \"GPT3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_df = pd.read_csv(f\"results_gpt3.csv\", sep = \",\")[[\"answer\", \"ground_truth\"]]\n",
    "print (gpt3_df.loc[0].answer)\n",
    "print (gpt3_df.loc[0].ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_df[[\"answer\", \"ground_truth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_RAG-EIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
